---
title: "University Acceptance Regression Model"
author: "Valencia Lie"
date: "6 July 2020"
output: 
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
    toc: true
    number_sections: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem at hand
This data set is taken from kaggle.com and contains several parameters which are considered important during the application for Masters Programs.
The parameters included are :

- GRE Scores ( out of 340 )
- TOEFL Scores ( out of 120 )
- University Rating ( out of 5 )
- Statement of Purpose and Letter of Recommendation Strength ( out of 5 )
- Undergraduate GPA ( out of 10 )
- Research Experience ( either 0 or 1 )
- Chance of Admit ( ranging from 0 to 1 )

In the report below, I will attempt to predict the chance of admittance of a student based on some of these parameters using regression model and will check how reliable my model is in predicting an outcome.

# Structure of this report
- Importing dataset and cleansing 
- Exploratory data analysis: to check correlation between these predictors and target (chance of admittance)
- Base model (step-wise regression)
- Building a proper model from the base model
- Predicting future data
- Check for errors, goodness of fit and violation of assumption
- Final model + conclusion


#Importing dataset and cleansing
```{r}
library(tidyverse)
library(lmtest)
library(MLmetrics)
library(GGally)
library(ggplot2)
```

```{r}
university <- read_csv("datasets_14872_228180_Admission_Predict_Ver1.1.csv")
```

```{r}
glimpse(university)
anyNA(university)
```


```{r}
university <- university %>% 
  select(- `Serial No.`) %>% 
  mutate(`Research` = as.factor(`Research`))
str(university)
```

#Exploratory Data Analysis
```{r}
ggcorr(university, label = T)
```
From the above diagram, we can tell that generally all predictors have a high positive correlation with the target (chance of admittance). However, the biggest seem to lie on Undergraduate GPA (0.9), followed by TOEFL score and GRE score (both 0.8).

Since the above diagram includes only numeric predictors, we will begin EDA for our one and only predictor that has a data type of factor.

```{r}
university %>% 
  group_by(`Research`) %>% 
  summarise(average_admit = mean(`Chance of Admit`)) %>% 
  ggplot(aes(x = `Research`, y = average_admit)) +
  geom_col(aes(fill = average_admit)) +
  labs(title = "Average chance of admittance score based on whether the student did research before or not",
       x = "Research (1 for yes, 0 for no)",
       y = "Average chance of admittance score",
       fill = "") 
```
From the above bar graph, we can see that when a student had research experience before, he/she has a higher average admittance chance compared to a student with no research experience before. Logically speaking, this makes sense because universities emphasise a lot on research, especially for a prestigious masters programme. Hence, an experience in research becomes a defining factor in a pool of equally talented students. 

#Base model (step-wise regression model)
## Backward regression model
```{r}
model_all <- lm(formula = `Chance of Admit` ~., university)
model_all
backward <- step(model_all, direction = "backward")
```
From the above, we can see that since the AIC is a negative value to begin with, the higher the absolute value (the more negative the loss in information), the better, and hence we are left with 6 predictors:
- University Rating
- TOEFL Score
- Research
- GRE Score
- LOR
- CGPA
with AIC of -2807.59

##Forward regression model 
```{r}
model_none <- lm(formula = `Chance of Admit` ~ 1, university)
forward <- step(model_none, direction = 'forward', scope = list(lower = model_none, upper = model_all))
```
For the forward model, we are left with the same 6 predictors too:
- CGPA 
- GRE Score 
- LOR
- Research
- TOEFL Score
- University Rating
with AIC of -2807.59

## Both forward and backward regression model

```{r}
both <- step(object = model_all, direction = "both", scope = list(lower = model_none, upper = model_all))
```

For the both model, we are left with 6 predictors too:
- GRE Score
- TOEFL Score 
- University Rating
- LOR 
- CGPA
- Research
with AIC of -2807.59

Overall, comparing all three base models, all have the same AIC and same 6 predictors. Hence, for simplicity sake, we will move forward with the backward model.

```{r}
summary(backward)
```

## Formula:

$$ Chance of admittance = -1.2800138 + 0.0018528(GRE Score) + 0.0028072(TOEFL Score) + 0.0064279(University Rating) + 0.0172873(LOR) + 0.1189994(CGPA) + 0.0243538(Research) $$

## Significance based on p value:
H0: Predictors do not have significance in predicting target (chance of admittance)
H1: Predictors have significance in predicting target (chance of admittance)
When p value is lower than 0.05, do not accept H0.

All predictors have p value of less than 0.05 except for University Rating (0.069363). This shows that University Rating does not have significance as opposed to all the other 5 predictors in predicting chance of admittance. Logically, this may seem plausible because although it's true that a university with a higher rating (or ranking) makes it harder for a student to get admitted through their more rigorous admittance process such as interviews and GPA cut-off, it is not always the case. 

For example:
According to this website: https://ingeniusprep.com/blog/college-acceptance-rates/, New York University is ranked 29 and has an acceptance rate of 14.5%, whereas University of Notre Dame is ranked much higher at 15 but has a higher acceptance rate of 15.4%. This may stem from the better reputation that NYU has compared to University of Notre Dame, allowing it to exercise a more stringent and rigorous admittance process. Although the above example is mainly for undergraduate programmes, it is likely to be the same with graduate programmes.

## Goodness of fit
This model has a multiple R-squared of 0.8219 and an adjusted R-squared of 0.8197. Since this is above 70%, this model is generally said to have a good fit.

#Building a proper model from the base model
From the above insight, I decided to use the same predictors from the backward model except for University rating in order to predict the chance of acceptance of students into the graduate programme.

```{r}
proper_model <- lm(`Chance of Admit` ~ `GRE Score` + `TOEFL Score` + LOR + CGPA + Research, university)
summary(proper_model)
```

Brief conclusion based on the summary:

## Formula
$$ Chance of admittance = -1.3357018 + 0.0018892(GRE Score) + 0.0030174(TOEFL Score) + 0.0193203(LOR) + 0.1229798(CGPA) + 0.0251649(Research) $$ 

##Significance of predictors
H0: Predictors do not have significance in predicting target (chance of admittance)
H1: Predictors have significance in predicting target (chance of admittance)
When p value is lower than 0.05, do not accept H0.

Since all predictors have p value of much less than 0.05, all predictors are said to be significant in predicting chance of admittance.

#Predicting of future data

Predicting using a 95% confidence level,
```{r}
university$prediction <- predict(proper_model, newdata = university, level = 0.95)
university
```

#Check for errors, goodness of fit and violation of assumption

##Calculation of RMSE and range of actual data
```{r}
x = RMSE(y_pred = university$prediction, y_true = university$`Chance of Admit`)
y = range(university$`Chance of Admit`)[1]
x
y
```

```{r}
y+x
y-x
```

Based on the calculation above, we can conclude that the RMSE (root mean square error) is 0.05971111. Since the lowest actual chance of admittance is 0.34 and the RMSE calculated is still much smaller than that, making the difference in the actual and predicted data to be very small, it can be concluded that this model is generally reliable in predicting future data.

## Goodness of fit
This model has a multiple R-squared of 0.8207 and an adjusted R-squared of 0.8188 and since the general threshold for a model with good fit is an R-squared of 70%, this model can be said to have a good fit.

##Checking for violation of assumption
### Normality in distribution of error

```{r}
hist(proper_model$residuals)
```
From the above histogram, we can tell that the errors of the model generally follow a bell-curved shape distribution, which is what we call normal distribution. However, to be more sure of this finding, I will further calculate using Shapiro-Wilk test.

Shapiro-Wilk hypothesis:
H0: The model's errors follow a normal distribution
H1: The model's error does not follow a normal distribution

```{r}
shapiro.test(proper_model$residuals)
```

Since p-value < 0.05, I must reject H0 and accept H1. Hence, this model's error does not follow a normal distribution.

#### Why must the errors of a model follow a normal distribution?
When errors of a model follow a standard normal distribution, its mean will be at 0 and the majority of the data of the error will be close to 0, making the model more reliable as the error will statistically be close to 0. Hence, we will try to make sure that the errors of our model follow a close resemblance of a normal distribution.

### Homoscedasticity

```{r}
plot(x = proper_model$fitted.values, y = proper_model$residuals)
abline(h = 0, col = 'red')
```
From this scatter plot, it is hard for us to tell whether the points are random and follow the assumption of homoscedasticity. Hence, we will try to calculate using Breusch-Pagan test.

```{r}
bptest(proper_model)
```

Breusch-Pagan test hypothesis:
- H0: Homoscedasticity
- H1: Heteroscedasticity

Since the p-value calculated is < 0.05, we must reject H0 and accept H1, meaning that it is heteroscedastic. 

#### Why must our model fulfill the assumption of being homoscedasticity?
When our model fulfills the assumption of being homoscedasticity, the scatter plot of our model's predicted value against our model's errors will not exhibit any pattern and will be random. This is needed because if it exhibits a pattern, it is likely that an improvement can be made to our model because the errors may not be considered random. Once the errors are completely random (and hence we cannot further predict them) is when our model is truly effective and reliable in predicting future data.

### No multi-collinearity
```{r}
#From `car` package, calculated using R studio cloud since my R version is too low to support `car`
#vif(proper_model)
```
GRE.Score TOEFL.Score         LOR        CGPA    Research 
   4.452473    3.799455    1.704623    4.376495    1.486588
   
Since the VIF for all the predictors are below 10, it can be said that these predictors generally have no multi-collinearity. 

#### Why must predictors have no multi-collinearity?
This is because when predictors have multi-collinearity, it is redundant to include both predictors in the first place. For example, if X is collinear with Y and both are used to predict Z (target), it is redundant to use X and Y together as X can be used to predict both Y (due to their collinearity) and Z alone. 
Other than this reason, it may even cause a confusing and ambiguous data because the reliability of the data predicted can be interfered by X and Y's collinearity.

#Final model + conclusion
Based on our checks, it can be said that our model is not up to standard as it only fulfills 1 out of the 3 assumptions, despite fulfilling the checks for errors and goodness of fit.

```{r}
shapiro.test(backward$residuals)
bptest(backward)
```

I have also checked the base model against these tests and found out that it, too, had failed 2 out of the 3 assumption checks. Though it fared better for the Breusch-Pagan test, it fared worse for the Shapiro-Wilk test.

We have seen that our current model is sub-par and tweaking needs to be done. One of the approaches that can be adopted is to shun off the variables that have correlation coefficient above 0.7.
```{r}
ggcorr(university, label = T)
```
Based on the correlation matrix, unfortunately, all the predictors have strong correlation with other variables. I have tried to tweak the models and it still could not fulfill the 3 assumptions as desired. The closest I could get is by using only LOR as a sole predictor since it has the weakest correlation with other variables.

```{r}
model_final <- lm(`Chance of Admit` ~ LOR, university)
shapiro.test(model_final$residuals)
bptest(model_final)
```
Although it fulfilled the homoscedasticity test, it could not fulfill the normality test. In addition, its adjusted R squared fared very badly (0.4165) and its RMSE is even higher than with the previous model (0.1077057 vs 0.05971111), making it an equally bad model.

```{r}
summary(model_final)
```

```{r}
university$prediction1 <- predict(object = model_final, university)
RMSE(y_pred = university$prediction1, y_true = university$`Chance of Admit`)
range(university$`Chance of Admit`)[1]
```

All in all, I would still choose to use either the proper model I constructed (not the final model) because it fit the problem we are dealing with the most or the base model because it has the least AIC. I can conclude that there is no better model that could have achieved the 3 assumption checks while simultaneously following the narrative we are aiming for. 

Furthermore, despite both models failing the normality test, I would not be too concerned regarding that because according to this Quora post:
https://www.quora.com/What-are-some-general-ways-to-improve-multiple-linear-regression-models,

"With sample sizes(Data) smaller than 15 the accuracy of the p-value is sensitive non-normal residuals errors. As sample sizes increase(>>15), the normality assumption for the residuals is not needed. More precisely, if we consider repeated sampling from our population, for large sample sizes, the distribution (across repeated samples) of the ordinary least squares estimates of the regression coefficients follow a normal distribution. As a consequence, for moderate to large sample sizes, non-normality of residuals should not adversely affect the usual inferential procedures."

Since our data has way more than 15, it should not be much of a problem if it does not fulfill the normality assumption. With that being said, one possible way to make this model a better model is to add more data to the data set. With more data, more can be accurately gauged by the model, leading to even more reliable predicted data.

In addition, regarding the homoscedasticity assumption, although the Breusch Pagan test revealed both models to be heteroscedastic, the scatter plot indicates no distinct pattern, making it ever so difficult for us to correct the model.

While it is important to constantly tweak a model in order to bring out the best possible outcome, it is vital to understand that no prediction can be 100% accurate and no model can ever be flawless. After all, life is as such and no amount of computation will be robust enough to predict a real-life target completely accurately.






